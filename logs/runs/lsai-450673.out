START TIME: Mon May 19 18:19:20 CEST 2025
Job Name: lsai
Job ID: 450673
Allocated Node(s): nid006670
Number of Tasks: 1
CPUs per Task: 72
GPUs per Node: 4
Number of Nodes: : 1
Number of Tasks per Node: 1
Current path: /workspaces
Current user: aoudrhiri
execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006670 noderank=0 localrank=0
[Distributed Init] Rank 0 initialized on 0 on GPU 0.
[rank0]:[W519 18:19:48.149964847 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Distributed Init] Rank 1 initialized on 0 on GPU 1.
[rank1]:[W519 18:19:48.211897296 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Distributed Init] Rank 2 initialized on 0 on GPU 2.
[rank2]:[W519 18:19:48.241875179 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Distributed Init] Rank 3 initialized on 0 on GPU 3.
[rank3]:[W519 18:19:48.303080800 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Rank 0] All ranks ready!
2025-05-19 18:19:56,512 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, experiment='tp_attention', user='aoudrhiri', tensor_parallel=True, data_parallel=False, tp_parallel_type='attention')
2025-05-19 18:19:56,512 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, experiment='tp_attention', user='aoudrhiri', tensor_parallel=True, data_parallel=False, tp_parallel_type='attention')
2025-05-19 18:19:56,512 - root - INFO - Setting up DataLoaders...
2025-05-19 18:19:56,512 - root - INFO - Setting up DataLoaders...
2025-05-19 18:19:56,512 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, experiment='tp_attention', user='aoudrhiri', tensor_parallel=True, data_parallel=False, tp_parallel_type='attention')
2025-05-19 18:19:56,513 - root - INFO - Setting up DataLoaders...
2025-05-19 18:19:56,513 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, experiment='tp_attention', user='aoudrhiri', tensor_parallel=True, data_parallel=False, tp_parallel_type='attention')
2025-05-19 18:19:56,513 - root - INFO - Setting up DataLoaders...
2025-05-19 18:20:01,179 - root - INFO - Setting up Model...
2025-05-19 18:20:01,179 - root - INFO - Setting up Model...
2025-05-19 18:20:01,179 - root - INFO - Setting up Model...
2025-05-19 18:20:01,179 - root - INFO - Setting up Model...
